{
 "cells": [
  {
   "cell_type": "code",
   "id": "ed81b61d-f249-47fe-b85b-641a9967a58b",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(\"Data/CustomerDataset_Q1.csv\")\n",
    "\n",
    "# Make the column name y instead of 'C' for plotting\n",
    "df.rename(columns={'c': 'y'}, inplace=True)\n",
    "\n",
    "# Part A\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df[df['y'] == 0]['x1'], df[df['y'] == 0]['x2'], color='blue', label='y=0 (Not interested)')\n",
    "plt.scatter(df[df['y'] == 1]['x1'], df[df['y'] == 1]['x2'], color='red', label='y=1 (Interested)')\n",
    "plt.xlabel(\"Average amount spent per purchase (x1)\")\n",
    "plt.ylabel(\"Frequency of purchases per month (x2)\")\n",
    "plt.title(\"Customer Data Scatter Plot\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Part B\n",
    "\n",
    "def euclidean_distance(point1, point2):\n",
    "    return math.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)\n",
    "\n",
    "def fnKNN(dataset, new_point, k):\n",
    "    distances = []\n",
    "    \n",
    "    for row in dataset:\n",
    "        x1, x2, label = row\n",
    "        dist = euclidean_distance([x1, x2], new_point)\n",
    "        distances.append((dist, label))\n",
    "    \n",
    "    # Sort distances and get k nearest neighbors\n",
    "    distances.sort()\n",
    "    k_nearest = distances[:k]\n",
    "    \n",
    "    # Majority vote\n",
    "    labels = [label for _, label in k_nearest]\n",
    "    prediction = max(set(labels), key=labels.count)\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Convert DataFrame to a list of lists\n",
    "data_list = df.values.tolist()\n",
    "\n",
    "# Part C\n",
    "\n",
    "def evaluate_knn(k, train_size):\n",
    "    random.seed(42)  # Ensure reproducibility\n",
    "    random.shuffle(data_list)\n",
    "    \n",
    "    train_count = int(len(data_list) * train_size)\n",
    "    train_set = data_list[:train_count]\n",
    "    test_set = data_list[train_count:]\n",
    "    \n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for test_point in test_set:\n",
    "        x1, x2, actual_label = test_point\n",
    "        predicted_label = fnKNN(train_set, [x1, x2], k)\n",
    "        \n",
    "        if predicted_label == actual_label:\n",
    "            correct_predictions += 1\n",
    "    \n",
    "    accuracy = correct_predictions / len(test_set) if test_set else 0\n",
    "    return accuracy\n",
    "    \n",
    "# Part D\n",
    "\n",
    "# Define different train-test splits\n",
    "splits = [0.8, 0.6, 0.5]\n",
    "k_values = [1, 2, 3, 4]\n",
    "accuracy_results = {}\n",
    "\n",
    "for k in k_values:\n",
    "    accuracy_results[k] = []\n",
    "    for split in splits:\n",
    "        accuracy = evaluate_knn(k, split)\n",
    "        accuracy_results[k].append(accuracy)\n",
    "\n",
    "# Print results in a fancy table format <3\n",
    "print(\"\\nAccuracy Results:\")\n",
    "print(f\"{'k':<5}{'80% Train':<12}{'60% Train':<12}{'50% Train':<12}\")\n",
    "for k, acc_list in accuracy_results.items():\n",
    "    print(f\"{k:<5}{acc_list[0]:<12.2%}{acc_list[1]:<12.2%}{acc_list[2]:<12.2%}\")\n",
    "\n",
    "# Just print the answers to the questions\n",
    "print(\"\"\"\n",
    "===== Summary of k-NN Classification Results =====\n",
    "\n",
    "d)   How does k affect performance?\n",
    "   - k = 1 overfits since it only considers the closest neighbor.\n",
    "   - k = 3 is more stable, reducing the impact of noise.\n",
    "   - Higher k (e.g., 4) can lead to misclassification due to ties.\n",
    "   - Lower k (e.g., 1) can lead to misclassification due to overfitting meaning misclassification due to presence of noise\n",
    "\n",
    "d)   How does training set size impact results?\n",
    "   - More training data generally improves accuracy.\n",
    "   - With 80% training data, both k = 1 and k = 3 reached 100% accuracy.\n",
    "   - Smaller training sets resulted in lower accuracy, showing the importance of data size.\n",
    "\n",
    "e)   Which combination seems best and why?\n",
    "   - k = 3 with all ratios of training data is the best choice.\n",
    "   - It balances high accuracy (100%, 75%, 80%) with better generalization compared to k = 1.\n",
    "    - k = 3 has a higher average accuracy\n",
    "\"\"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a81e552-dae3-4943-bc39-e297b8668f57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
